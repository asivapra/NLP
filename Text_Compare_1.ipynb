{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<spacy.lang.en.English object at 0x000002D052F4C5C8>\n",
      "https://en.wikipedia.org/wiki/Star_Wars   https://en.wikipedia.org/wiki/Star_Wars\n",
      "127 127\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "import urllib.request\n",
    "from bs4 import BeautifulSoup\n",
    "from bs4.element import Comment\n",
    "import urllib.request\n",
    "import re\n",
    "\n",
    "# Load only once\n",
    "try:\n",
    "    print(nlp)\n",
    "except:\n",
    "    nlp = spacy.load(\"en_core_web_md\")\n",
    "\n",
    "\n",
    "\n",
    "# Only the visible text is to be retrieved\n",
    "def tag_visible(element):\n",
    "    if element.parent.name in ['style', 'script', 'head', 'title', 'meta', '[document]']:\n",
    "        return False\n",
    "    if isinstance(element, Comment):\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "def text_from_html(body):\n",
    "    soup = BeautifulSoup(body, 'html.parser')\n",
    "    texts = soup.findAll(text=True)\n",
    "    visible_texts = filter(tag_visible, texts)  \n",
    "    title = soup.title.string\n",
    "    return u\" \".join(t.strip() for t in visible_texts), title\n",
    "\n",
    "def tokenize(text):\n",
    "    t0 = []\n",
    "    # Split the text into words separated by any of these chars\n",
    "    p = re.compile('[-_\\\", {};:=?\\[\\]\\(\\)\\'.]')\n",
    "    \n",
    "    # Get the word list into an array, t\n",
    "    t = p.split(text)\n",
    "    lt = len(t)\n",
    "\n",
    "    # Add the 4+ chars words into an array, t1. Discard the smaller words\n",
    "    for i in range(lt):\n",
    "        w = t[i]\n",
    "        lw = len(w)\n",
    "        if(lw > 3):\n",
    "            t0.append(w)\n",
    "    return t0\n",
    "\n",
    "def Lemmatise(text):\n",
    "# Implementing lemmatization\n",
    "    lem = nlp(text)\n",
    "    # finding lemma for each word\n",
    "    for word in lem:\n",
    "        print(word.text,word.lemma_)\n",
    "    pass\n",
    "def ReadUrls(i, j):\n",
    "    url1 = urls[i]\n",
    "    url2 = urls[j]\n",
    "    global html1, html2, t1, t2\n",
    "    t1 = []\n",
    "    t2 = []\n",
    "    response =  urllib.request.urlopen(url1)\n",
    "    html1 = response.read()\n",
    "    response =  urllib.request.urlopen(url2)\n",
    "    html2 = response.read()\n",
    "    text1, title1 = text_from_html(html1)\n",
    "    text1 = Lemmatise(text1)\n",
    "    text2, title2 = text_from_html(html2)\n",
    "    t1 = tokenize(text1)\n",
    "    t2 = tokenize(text2)  \n",
    "    print(url1, ' ', url2)        \n",
    "\n",
    "def GetDocSegments():\n",
    "    global doc1_segs, doc2_segs, l, m\n",
    "    # Split and save the text as 100 word segments\n",
    "    doc1_segs = []\n",
    "    doc2_segs = []\n",
    "    text = ''\n",
    "    # Split t1 and t2 into text of 100 words\n",
    "    l = len(t1)\n",
    "    n = int(l/100)\n",
    "    for i in range(n):\n",
    "        b = i*100\n",
    "        e = b+100\n",
    "        for j in range(b,e):\n",
    "            text += t1[j] + ' '\n",
    "        doc1_segs.append(text)\n",
    "        text = ''\n",
    "\n",
    "    l = len(t2)\n",
    "    n = int(l/100)\n",
    "    for i in range(n):\n",
    "        b = i*100\n",
    "        e = b+100\n",
    "        for j in range(b,e):\n",
    "            text += t2[j] + ' '\n",
    "        doc2_segs.append(text)\n",
    "        text = ''\n",
    "\n",
    "    l = len(doc1_segs)    \n",
    "    m = len(doc2_segs) \n",
    "    print(l,m)\n",
    "\n",
    "def CalculateSimilarity():\n",
    "    sim = 0.00\n",
    "    sims = []\n",
    "    tot = 0.00\n",
    "    k = 10\n",
    "    if (l < k):\n",
    "        k = l\n",
    "    if (m < k):\n",
    "        k = m\n",
    "    for i in range(k):\n",
    "        for j in range(k):\n",
    "            doc1 = nlp(doc1_segs[i])\n",
    "            doc2 = nlp(doc2_segs[j])\n",
    "            sim = doc1.similarity(doc2)\n",
    "            sims.append(sim)\n",
    "    sims.sort(reverse=True)\n",
    "    for i in range(k):\n",
    "        tot += sims[i]\n",
    "    avg = tot/k\n",
    "    print(avg)\n",
    "\n",
    "# Globals\n",
    "urls = [\n",
    "\"https://en.wikipedia.org/wiki/Machine_learning\", # 0\n",
    "\"https://www.sas.com/en_au/insights/analytics/machine-learning.html\", # 1\n",
    "\"https://en.wikipedia.org/wiki/SpaceX\", # 2\n",
    "\"https://en.wikipedia.org/wiki/London\", # 3\n",
    "\"https://en.wikipedia.org/wiki/England\",# 4\n",
    "\"https://www.webgenie.com/details.html\", # 5\n",
    "\"https://en.wikipedia.org/wiki/Titanic_(1997_film)\", # 6\n",
    "\"https://en.wikipedia.org/wiki/Titanic_(1953_film)\", # 7\n",
    "\"https://simple.wikipedia.org/wiki/Titanic_(1997_movie)\", # 8\n",
    "\"https://en.wikipedia.org/wiki/Star_Wars\", # 9\n",
    "\"https://en.wikipedia.org/wiki/List_of_Star_Wars_films\" # 10\n",
    "]\n",
    "\n",
    "\n",
    "ReadUrls(9, 9)\n",
    "GetDocSegments()\n",
    "CalculateSimilarity()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
