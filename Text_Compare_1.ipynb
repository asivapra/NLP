{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import urllib.request\n",
    "from bs4 import BeautifulSoup\n",
    "from bs4.element import Comment\n",
    "import urllib.request\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "# Load only once\n",
    "try:\n",
    "    if(nlp):\n",
    "        pass\n",
    "except:\n",
    "    nlp = spacy.load(\"en_core_web_md\")\n",
    "\n",
    "# Load only once\n",
    "try:\n",
    "    if(sr[0]):\n",
    "        pass\n",
    "except:\n",
    "    sr = stopwords.words('english')\n",
    "\n",
    "\n",
    "# Only the visible text is to be retrieved\n",
    "def tag_visible(element):\n",
    "    if element.parent.name in ['style', 'script', 'head', 'title', 'meta', '[document]']:\n",
    "        return False\n",
    "    if isinstance(element, Comment):\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "def text_from_html(body):\n",
    "    soup = BeautifulSoup(body, 'html.parser')\n",
    "    texts = soup.findAll(text=True)\n",
    "    visible_texts = filter(tag_visible, texts)  \n",
    "    title = soup.title.string\n",
    "    return u\" \".join(t.strip() for t in visible_texts), title\n",
    "\n",
    "def tokenize(text):\n",
    "    t0 = []\n",
    "    # Split the text into words separated by any of these chars\n",
    "    p = re.compile('[-_\\\", {};:=?\\[\\]\\(\\)\\'.]')\n",
    "    \n",
    "    # Get the word list into an array, t\n",
    "    t = p.split(text)\n",
    "    lt = len(t)\n",
    "\n",
    "    # Add the 4+ chars words into an array, t1. Discard the smaller words\n",
    "    for i in range(lt):\n",
    "        w = t[i]\n",
    "        lw = len(w)\n",
    "        if(lw > 3):\n",
    "            t0.append(w)\n",
    "    return t0\n",
    "\n",
    "def Lemmatise(text):\n",
    "# Implementing lemmatization\n",
    "    p = re.compile('[a-zA-Z]')\n",
    "    lt = []\n",
    "    lem = nlp(text)\n",
    "    # finding lemma for each word\n",
    "    for word in lem:\n",
    "#        print(word.text, word.lemma_)\n",
    "        if(p.findall(str(word))):\n",
    "            lemma = word.lemma_.lower() \n",
    "            if(len(lemma) > 3):\n",
    "                if str(lemma) not in (sr):\n",
    "                    lt.append(lemma)\n",
    "#    print(t1)\n",
    "    return lt\n",
    "def CountWords(t):\n",
    "    global counts\n",
    "    from collections import Counter\n",
    "    counts = Counter(t)\n",
    "#    print(counts)\n",
    "\n",
    "def ReadUrls(i, j):\n",
    "    url1 = urls[i]\n",
    "    url2 = urls[j]\n",
    "#    print(urls[i], urls[j])\n",
    "    global html1, html2, t1, t2, t1w, t2w, t1t, t2t\n",
    "    t1 = []\n",
    "    t2 = []\n",
    "    response =  urllib.request.urlopen(url1)\n",
    "    html1 = response.read()\n",
    "    response =  urllib.request.urlopen(url2)\n",
    "    html2 = response.read()\n",
    "    text1, title1 = text_from_html(html1)\n",
    "    t1 = Lemmatise(text1)\n",
    "    t1t = Lemmatise(str(title1))\n",
    "    text2, title2 = text_from_html(html2)\n",
    "    t2 = Lemmatise(text2)\n",
    "    t2t = Lemmatise(str(title2))\n",
    "\n",
    "    # Make a list of words and their counts. We will take the top 10 for further analysis\n",
    "    CountWords(t1)\n",
    "    t1w = []\n",
    "    for key,val in counts.items():\n",
    "        item = str(val) + ':' + str(key.lower())\n",
    "        t1w.append(item)\n",
    "    t1w.sort(key=lambda fname: int(fname.split(':')[0]), reverse=True)\n",
    "    CountWords(t2)\n",
    "    t2w = []\n",
    "    for key,val in counts.items():\n",
    "        item = str(val) + ':' + str(key.lower())\n",
    "        t2w.append(item)\n",
    "    t2w.sort(key=lambda fname: int(fname.split(':')[0]), reverse=True)\n",
    "    \n",
    "def GetDocSegments():\n",
    "    global doc1_segs, doc2_segs, l, m\n",
    "    # Split and save the text as 100 word segments\n",
    "    doc1_segs = []\n",
    "    doc2_segs = []\n",
    "    text = ''\n",
    "    # Split t1 and t2 into text of 100 words\n",
    "    l = len(t1)\n",
    "    n = int(l/100)\n",
    "    for i in range(n):\n",
    "        b = i*100\n",
    "        e = b+100\n",
    "        for j in range(b,e):\n",
    "            text += t1[j] + ' '\n",
    "        doc1_segs.append(text)\n",
    "        text = ''\n",
    "\n",
    "    l = len(t2)\n",
    "    n = int(l/100)\n",
    "    for i in range(n):\n",
    "        b = i*100\n",
    "        e = b+100\n",
    "        for j in range(b,e):\n",
    "            text += t2[j] + ' '\n",
    "        doc2_segs.append(text)\n",
    "        text = ''\n",
    "\n",
    "    l = len(doc1_segs)    \n",
    "    m = len(doc2_segs) \n",
    "#    print(l,m) # The number of 100-word strings in each\n",
    "\n",
    "def CalculateSimilarity(i,j,nurls):\n",
    "    sim = 0.00\n",
    "    sims = []\n",
    "    tot = 0.00\n",
    "    k = nurls\n",
    "    if (l < k):\n",
    "        k = l\n",
    "    if (m < k):\n",
    "        k = m\n",
    "    if(k == 0):\n",
    "        return 0\n",
    "    for i in range(k):\n",
    "        for j in range(k):\n",
    "            doc1 = nlp(doc1_segs[i])\n",
    "            doc2 = nlp(doc2_segs[j])\n",
    "            sim = doc1.similarity(doc2)\n",
    "            sims.append(sim)\n",
    "    sims.sort(reverse=True)\n",
    "    for i in range(k):\n",
    "        tot += sims[i]\n",
    "    avg = tot/k\n",
    "    return avg\n",
    "#    print(avg)\n",
    "\n",
    "def CalculateWordFrequency():\n",
    "    tw1f = []\n",
    "    tw2f = []\n",
    "# ------------t1--------------\n",
    "    lw = len(t1w)\n",
    "    tot = 0\n",
    "    # Count the total number of words\n",
    "    for i in range (0,3):\n",
    "        fields = t1w[i].split(\":\")\n",
    "        tot += int(fields[0])\n",
    "    # Take the percentage of top 10 words in t1w\n",
    "    for i in range (0,3):\n",
    "        fields = t1w[i].split(\":\")\n",
    "        c = int(fields[0])\n",
    "        w = fields[1]\n",
    "        f = str(round(float(c/tot*100),2))\n",
    "        fw = f + \":\" + w\n",
    "        tw1f.append(fw)\n",
    "# ------------t2--------------\n",
    "    lw = len(t2w)\n",
    "    tot = 0\n",
    "    # Count the total number of words\n",
    "    for i in range (0,3):\n",
    "        fields = t2w[i].split(\":\")\n",
    "        tot += int(fields[0])\n",
    "    for i in range (0,3):\n",
    "        fields = t2w[i].split(\":\")\n",
    "        c = int(fields[0])\n",
    "        w = fields[1]\n",
    "        f = str(round(float(c/tot*100),2))\n",
    "        fw = f + \":\" + w\n",
    "        tw2f.append(w)\n",
    "# Compare the two to get the word frequency\n",
    "    lw1 = len(tw1f)\n",
    "    lw2 = len(tw2f)\n",
    "    mf = 0.00\n",
    "    for i in range(lw2):\n",
    "        w2 = tw2f[i]\n",
    "        tf = 0.00\n",
    "        for j in range(lw1):\n",
    "            fields = tw1f[j].split(\":\")\n",
    "            f1 = float(fields[0])\n",
    "            w1 = fields[1]\n",
    "            tf += f1\n",
    "            if(w1 == w2):\n",
    "#                print(w1,w2)\n",
    "                mf += f1\n",
    "    freq = round(mf/tf,2)\n",
    "#    print(mf, tf, freq)\n",
    "            \n",
    "#    print(tw1f)\n",
    "#    print(tw2f)\n",
    "    return freq\n",
    "#        wordfreq += fw + \" \"\n",
    "#    print(wordfreq)\n",
    "    \n",
    "# Globals\n",
    "urls_0 = [\n",
    "\"https://en.wikipedia.org/wiki/Machine_learning\", # 0\n",
    "\"https://www.sas.com/en_au/insights/analytics/machine-learning.html\", # 1\n",
    "\"https://en.wikipedia.org/wiki/SpaceX\", # 2\n",
    "\"https://en.wikipedia.org/wiki/London\", # 3\n",
    "\"https://en.wikipedia.org/wiki/England\",# 4\n",
    "\"https://www.webgenie.com/details.html\", # 5\n",
    "\"https://en.wikipedia.org/wiki/Titanic_(1997_film)\", # 6\n",
    "\"https://en.wikipedia.org/wiki/Titanic_(1953_film)\", # 7\n",
    "\"https://simple.wikipedia.org/wiki/Titanic_(1997_movie)\", # 8\n",
    "\"https://en.wikipedia.org/wiki/Star_Wars\", # 9\n",
    "\"https://en.wikipedia.org/wiki/List_of_Star_Wars_films\" # 10\n",
    "]\n",
    "urls = [\n",
    "\"https://www.ands.org.au/working-with-data/metadata/geospatial-data-and-metadata\",#0\n",
    "\"https://www.ands.org.au/guides/geospatial\",#1\n",
    "\"https://www.gislounge.com/difference-gis-geospatial/\",#2\n",
    "\"https://gisgeography.com/what-is-geodata-geospatial-data/\",#3\n",
    "\"https://www.mathworks.com/help/map/what-is-geospatial-data.html\",#4\n",
    "\"https://unimelb.libguides.com/GIS\",#5\n",
    "\"https://www.veris.com.au/our-services/geospatial-data-management/\"#6\n",
    "]\n",
    "nurls = len(urls)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Title_Match(t1t, t2t):\n",
    "    tv = 0.00\n",
    "    for w1 in (t1t):\n",
    "        for w2 in (t2t):\n",
    "            if (w1 == w2):\n",
    "                tv = 0.1\n",
    "    return tv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Domain_match(url1, url2):\n",
    "    dv = 0.00\n",
    "    f1 = url1.split('/')\n",
    "    d1 = f1[2].replace('www.','')\n",
    "    f2 = url2.split('/')\n",
    "    d2 = f2[2].replace('www.','')\n",
    "    if(d1 == d2):\n",
    "        dv = 0.1\n",
    "    return dv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['geospatial', 'datum', 'metadata'] ['geographic', 'information', 'systems', 'system', 'services', 'veris', 'australia'] 0.0 0.0\n",
      "Score:  0.95 out of 2.00 ( 47.5 %)\n"
     ]
    }
   ],
   "source": [
    "#-----------------------------------------------------------------------------------------------------------------\n",
    "# Call the functions\n",
    "# Read the HTML contents and lemmatise into two arrays, 't1' and 't2'\n",
    "i = 0\n",
    "j = 6\n",
    "ReadUrls(i, j)\n",
    "#print(t1w[:10])\n",
    "\n",
    "# Concatenate 100 words each from t1 and t2 into separate strings and add to two arrays, 'doc1_segs' and 'doc2_segs'\n",
    "GetDocSegments()\n",
    "\n",
    "# Calculate the cosine similarity\n",
    "sim = CalculateSimilarity(i,j,nurls)\n",
    "\n",
    "freq = CalculateWordFrequency()\n",
    "tv = Title_Match(t1t, t2t) # If any word in titles match, tf=0.1\n",
    "dv = Domain_match(urls[i], urls[j])\n",
    "s = round((sim + freq + tv + dv),2)\n",
    "#s = round((sim + freq),2)\n",
    "sp = round(s/2.00*100.0,2)\n",
    "print(t1t, t2t, tv, dv)\n",
    "print('Score: ', s, 'out of 2.00 (', sp , '%)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#\t0\t1\t2\t3\t4\t5\t6\t\n",
      "0\t2.2\t1.88\t1.05\t1.52\t1.04\t1.53\t0.95\t\n",
      "1\t\t2.2\t1.32\t1.61\t1.32\t1.63\t1.21\t\n",
      "2\t\t\t2.2\t1.27\t1.24\t1.27\t1.69\t\n",
      "3\t\t\t\t2.2\t1.62\t1.83\t1.52\t\n",
      "4\t\t\t\t\t2.2\t1.35\t1.23\t\n",
      "5\t\t\t\t\t\t2.2\t1.55\t\n",
      "6\t\t\t\t\t\t\t2.2\t\n"
     ]
    }
   ],
   "source": [
    "h = \"#\\t\"\n",
    "o = \"\"\n",
    "for i in range (nurls):\n",
    "    h += str(i) + '\\t'\n",
    "#h += '\\n'  \n",
    "print(h)\n",
    "#print('Blah')\n",
    "def AddTabs(i):\n",
    "    global o\n",
    "    for j in range(i):\n",
    "        o += '\\t'\n",
    "for i in range (nurls):\n",
    "    o = str(i)\n",
    "    AddTabs(i+1)\n",
    "    for j in range (i,nurls):\n",
    "        ReadUrls(i, j)\n",
    "        tv = Title_Match(t1t, t2t) # If any word in titles match, tf=0.1\n",
    "        dv = Domain_match(urls[i], urls[j])\n",
    "        GetDocSegments()\n",
    "        sim = CalculateSimilarity(i,j,nurls)\n",
    "        freq = CalculateWordFrequency()\n",
    "        s = round((sim + freq + tv + dv),2)\n",
    "        o += str(s) + '\\t'\n",
    "#    o += '\\n'  \n",
    "    print(o)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
