{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import urllib.request\n",
    "from bs4 import BeautifulSoup\n",
    "from bs4.element import Comment\n",
    "import urllib.request\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "# Load only once\n",
    "try:\n",
    "    if(nlp):\n",
    "        pass\n",
    "except:\n",
    "    nlp = spacy.load(\"en_core_web_md\")\n",
    "\n",
    "# Load only once\n",
    "try:\n",
    "    if(sr[0]):\n",
    "        pass\n",
    "except:\n",
    "    sr = stopwords.words('english')\n",
    "\n",
    "\n",
    "def tokenize(text):\n",
    "    t0 = []\n",
    "    # Split the text into words separated by any of these chars\n",
    "    p = re.compile('[-_\\\", {};:=?\\[\\]\\(\\)\\'.]')\n",
    "    \n",
    "    # Get the word list into an array, t\n",
    "    t = p.split(text)\n",
    "    lt = len(t)\n",
    "\n",
    "    # Add the 4+ chars words into an array, t1. Discard the smaller words\n",
    "    for i in range(lt):\n",
    "        w = t[i]\n",
    "        lw = len(w)\n",
    "        if(lw > 3):\n",
    "            t0.append(w)\n",
    "    return t0\n",
    "\n",
    "def Lemmatise(text):\n",
    "# Implementing lemmatization\n",
    "    p = re.compile('[a-zA-Z]')\n",
    "    lt = []\n",
    "    lem = nlp(text)\n",
    "    # finding lemma for each word\n",
    "    for word in lem:\n",
    "#        print(word.text, word.lemma_)\n",
    "        if(p.findall(str(word))):\n",
    "            lemma = word.lemma_.lower() \n",
    "            if(len(lemma) > 3):\n",
    "                if str(lemma) not in (sr):\n",
    "                    lt.append(lemma)\n",
    "#    print(t1)\n",
    "    return lt\n",
    "def CountWords(t):\n",
    "    global counts\n",
    "    from collections import Counter\n",
    "    counts = Counter(t)\n",
    "#    print(counts)\n",
    "\n",
    "def ReadUrls(i, j):\n",
    "    url1 = urls[i]\n",
    "    url2 = urls[j]\n",
    "#    print(urls[i], urls[j])\n",
    "    global html1, html2, t1, t2, t1w, t2w, t1t, t2t, kw1, desc1, kw2, desc2, title1, title2\n",
    "    t1 = []\n",
    "    t2 = []\n",
    "    response =  urllib.request.urlopen(url1)\n",
    "    html1 = response.read()\n",
    "    response =  urllib.request.urlopen(url2)\n",
    "    html2 = response.read()\n",
    "    text1, title1, kw1, desc1 = text_from_html(html1)\n",
    "    t1 = Lemmatise(text1)\n",
    "    t1t = Lemmatise(str(title1))\n",
    "    text2, title2, kw2, desc2 = text_from_html(html2)\n",
    "    t2 = Lemmatise(text2)\n",
    "    t2t = Lemmatise(str(title2))\n",
    "\n",
    "    # Make a list of words and their counts. We will take the top 10 for further analysis\n",
    "    CountWords(t1)\n",
    "    t1w = []\n",
    "    for key,val in counts.items():\n",
    "        item = str(val) + ':' + str(key.lower())\n",
    "        t1w.append(item)\n",
    "    t1w.sort(key=lambda fname: int(fname.split(':')[0]), reverse=True)\n",
    "    CountWords(t2)\n",
    "    t2w = []\n",
    "    for key,val in counts.items():\n",
    "        item = str(val) + ':' + str(key.lower())\n",
    "        t2w.append(item)\n",
    "    t2w.sort(key=lambda fname: int(fname.split(':')[0]), reverse=True)\n",
    "    \n",
    "def GetDocSegments():\n",
    "    # Concatenate 100 words each from t1 and t2 into separate strings and add to two arrays, 'doc1_segs' and 'doc2_segs'\n",
    "    global doc1_segs, doc2_segs\n",
    "    # Split and save the text as 100 word segments\n",
    "    doc1_segs = []\n",
    "    doc2_segs = []\n",
    "    text = ''\n",
    "    # Split t1 and t2 into text of 100 words\n",
    "    l = len(t1)\n",
    "    n = int(l/100)\n",
    "    for i in range(n):\n",
    "        b = i*100\n",
    "        e = b+100\n",
    "        for j in range(b,e):\n",
    "            text += t1[j] + ' '\n",
    "        doc1_segs.append(text)\n",
    "        text = ''\n",
    "\n",
    "    l = len(t2)\n",
    "    n = int(l/100)\n",
    "    for i in range(n):\n",
    "        b = i*100\n",
    "        e = b+100\n",
    "        for j in range(b,e):\n",
    "            text += t2[j] + ' '\n",
    "        doc2_segs.append(text)\n",
    "        text = ''\n",
    "\n",
    "    l = len(doc1_segs)    \n",
    "    m = len(doc2_segs) \n",
    "    return l,m\n",
    "\n",
    "def CalculateSimilarity(i,j,nurls):\n",
    "    l,m = GetDocSegments() # Split the doc into small chunks\n",
    "    sim = 0.00\n",
    "    sims = []\n",
    "    tot = 0.00\n",
    "    k = nurls\n",
    "    if (l < k):\n",
    "        k = l\n",
    "    if (m < k):\n",
    "        k = m\n",
    "    if(k == 0):\n",
    "        return 0\n",
    "    for i in range(k):\n",
    "        for j in range(k):\n",
    "            doc1 = nlp(doc1_segs[i])\n",
    "            doc2 = nlp(doc2_segs[j])\n",
    "            sim = doc1.similarity(doc2)\n",
    "            sims.append(sim)\n",
    "    sims.sort(reverse=True)\n",
    "    for i in range(k):\n",
    "        tot += sims[i]\n",
    "    avg = round(tot/k,2)\n",
    "    return avg\n",
    "\n",
    "def CalculateWordFrequency():\n",
    "    tw1f = []\n",
    "    tw2f = []\n",
    "    nw = 10 # Number of words in the list to be used\n",
    "# ------------t1--------------\n",
    "    lw = len(t1w)\n",
    "    tot = 0\n",
    "    # Count the total number of words\n",
    "    for i in range (0,nw):\n",
    "        fields = t1w[i].split(\":\")\n",
    "        tot += int(fields[0])\n",
    "    # Take the percentage of top 10 words in t1w\n",
    "    for i in range (0,nw):\n",
    "        fields = t1w[i].split(\":\")\n",
    "        c = int(fields[0])\n",
    "        w = fields[1]\n",
    "        f = str(round(float(c/tot*100),2))\n",
    "        fw = f + \":\" + w\n",
    "        tw1f.append(fw)\n",
    "# ------------t2--------------\n",
    "    lw = len(t2w)\n",
    "    tot = 0\n",
    "    # Count the total number of words\n",
    "    for i in range (0,nw):\n",
    "        fields = t2w[i].split(\":\")\n",
    "        tot += int(fields[0])\n",
    "    for i in range (0,nw):\n",
    "        fields = t2w[i].split(\":\")\n",
    "        c = int(fields[0])\n",
    "        w = fields[1]\n",
    "        f = str(round(float(c/tot*100),2))\n",
    "        fw = f + \":\" + w\n",
    "        tw2f.append(w)\n",
    "# Compare the two to get the word frequency\n",
    "    lw1 = len(tw1f)\n",
    "    lw2 = len(tw2f)\n",
    "    mf = 0.00\n",
    "    for i in range(lw2):\n",
    "        w2 = tw2f[i]\n",
    "        tf = 0.00\n",
    "        for j in range(lw1):\n",
    "            fields = tw1f[j].split(\":\")\n",
    "            f1 = float(fields[0])\n",
    "            w1 = fields[1]\n",
    "            tf += f1\n",
    "            if(w1 == w2):\n",
    "                mf += f1\n",
    "    freq = round(mf/tf,2)\n",
    "    return freq\n",
    "\n",
    "def Title_Match(t1t, t2t):\n",
    "    tv = 0.00\n",
    "    for w1 in (t1t):\n",
    "        for w2 in (t2t):\n",
    "            if (w1 == w2):\n",
    "                tv = 0.1\n",
    "    return tv\n",
    "\n",
    "def Domain_match(url1, url2):\n",
    "    dv = 0.00\n",
    "    f1 = url1.split('/')\n",
    "    d1 = f1[2].replace('www.','')\n",
    "    f2 = url2.split('/')\n",
    "    d2 = f2[2].replace('www.','')\n",
    "    if(d1 == d2):\n",
    "        dv = 1.0\n",
    "    return dv\n",
    "\n",
    "# Only the visible text is to be retrieved\n",
    "def tag_visible(element):\n",
    "    if element.parent.name in ['a', 'link', 'style', 'script', 'head', 'title', 'meta', '[document]']:\n",
    "        return False\n",
    "    if isinstance(element, Comment):\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "def text_from_html(body):\n",
    "    soup = BeautifulSoup(body, 'html.parser')\n",
    "    keywords = ''\n",
    "    description = ''\n",
    "    title = soup.title.string\n",
    "    for tag in soup.find_all(\"meta\"):\n",
    "        if tag.get(\"name\", None) == \"og:title\":\n",
    "            title = tag.get(\"content\", None)\n",
    "            pass\n",
    "        if tag.get(\"name\", None) == \"keywords\":\n",
    "            keywords = tag.get(\"content\", None)\n",
    "            pass\n",
    "    \n",
    "        if tag.get(\"name\", None) == \"description\":\n",
    "            description = tag.get(\"content\", None)\n",
    "            pass\n",
    "    \n",
    "    texts = soup.findAll(text=True)\n",
    "    visible_texts = filter(tag_visible, texts)  \n",
    "    return u\" \".join(t.strip() for t in visible_texts), title, keywords, description\n",
    "\n",
    "def SimilarityScore(t1, t2):\n",
    "    doc1 = nlp(t1)\n",
    "    doc2 = nlp(t2)\n",
    "    sim = round(doc1.similarity(doc2),2)\n",
    "    return sim\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Topic: Variable\n",
    "urls_0 = [\n",
    "\"https://en.wikipedia.org/wiki/Machine_learning\", # 0\n",
    "\"https://www.sas.com/en_au/insights/analytics/machine-learning.html\", # 1\n",
    "\"https://en.wikipedia.org/wiki/SpaceX\", # 2\n",
    "\"https://en.wikipedia.org/wiki/London\", # 3\n",
    "\"https://en.wikipedia.org/wiki/England\",# 4\n",
    "\"https://www.webgenie.com/details.html\", # 5\n",
    "\"https://en.wikipedia.org/wiki/Titanic_(1997_film)\", # 6\n",
    "\"https://en.wikipedia.org/wiki/Titanic_(1953_film)\", # 7\n",
    "\"https://simple.wikipedia.org/wiki/Titanic_(1997_movie)\", # 8\n",
    "\"https://en.wikipedia.org/wiki/Star_Wars\", # 9\n",
    "\"https://en.wikipedia.org/wiki/List_of_Star_Wars_films\", # 10\n",
    "]\n",
    "# Topic: Geo-spatial data\n",
    "urls_1 = [\n",
    "    \"https://www.ands.org.au/working-with-data/metadata/geospatial-data-and-metadata\",\n",
    "    \"https://www.ands.org.au/guides/geospatial\",\n",
    "    \"https://www.safe.com/what-is/spatial-data/\",\n",
    "    \"https://gisgeography.com/what-is-geodata-geospatial-data/\",\n",
    "    \"https://www.mathworks.com/help/map/what-is-geospatial-data.html\",\n",
    "    \"https://www.veris.com.au/our-services/geospatial-data-management/\",\n",
    "]\n",
    "# Topic: Natural Language Processing\n",
    "urls_2 = [\n",
    "    \"https://searchbusinessanalytics.techtarget.com/definition/natural-language-processing-NLP\",\n",
    "    \"https://algorithmia.com/blog/introduction-natural-language-processing-nlp\",\n",
    "    \"https://www.coursera.org/learn/language-processing\",\n",
    "    \"https://en.wikipedia.org/wiki/Natural_language_processing\",\n",
    "    \"https://www.forbes.com/sites/bernardmarr/2019/06/03/5-amazing-examples-of-natural-language-processing-nlp-in-practice/\",\n",
    "    \"https://www.sas.com/en_au/insights/analytics/what-is-natural-language-processing-nlp.html\",\n",
    "]\n",
    "# Topic: Australian Indigenous Languages\n",
    "urls_3 = [\n",
    "    \"https://aiatsis.gov.au/explore/articles/indigenous-australian-languages\",\n",
    "    \"https://en.wikipedia.org/wiki/Australian_Aboriginal_languages\",\n",
    "    \"https://www.commonground.org.au/learn/indigenous-languages-avoiding-a-silent-future\",\n",
    "    \"http://theconversation.com/the-state-of-australias-indigenous-languages-and-how-we-can-help-people-speak-them-more-often-109662\",\n",
    "    \"https://www.clc.org.au/articles/info/aboriginal-languages/\",\n",
    "    \"https://www.britannica.com/topic/Australian-Aboriginal-languages\",   \n",
    "]\n",
    "# Topic: Human genome sequence\n",
    "urls_4 = [\n",
    "    \"https://en.wikipedia.org/wiki/Human_genome\",\n",
    "    \"https://en.wikipedia.org/wiki/Human_Genome_Project\",\n",
    "    \"https://www.nature.com/scitable/topicpage/dna-sequencing-technologies-key-to-the-human-828/\",\n",
    "    \"https://www.britannica.com/event/Human-Genome-Project/Advances-based-on-the-HGP\",\n",
    "    \"https://www.genome.gov/human-genome-project/Completion-FAQ\",\n",
    "    \"https://www.yourgenome.org/stories/how-is-the-completed-human-genome-sequence-being-used\",\n",
    "    \n",
    "    \n",
    "]\n",
    "nurls = len(urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Topics: Variable\n",
    "urls = [\n",
    "    \"https://www.ands.org.au/working-with-data/metadata/geospatial-data-and-metadata\",\n",
    "    \"https://www.ands.org.au/guides/geospatial\",\n",
    "    \"https://www.safe.com/what-is/spatial-data/\",\n",
    "    \"https://gisgeography.com/what-is-geodata-geospatial-data/\",\n",
    "    \"https://www.mathworks.com/help/map/what-is-geospatial-data.html\",\n",
    "    \"https://www.veris.com.au/our-services/geospatial-data-management/\",\n",
    "\n",
    "    \"https://searchbusinessanalytics.techtarget.com/definition/natural-language-processing-NLP\",\n",
    "    \"https://algorithmia.com/blog/introduction-natural-language-processing-nlp\",\n",
    "    \"https://www.coursera.org/learn/language-processing\",\n",
    "    \"https://en.wikipedia.org/wiki/Natural_language_processing\",\n",
    "    \"https://www.forbes.com/sites/bernardmarr/2019/06/03/5-amazing-examples-of-natural-language-processing-nlp-in-practice/\",\n",
    "    \"https://www.sas.com/en_au/insights/analytics/what-is-natural-language-processing-nlp.html\",\n",
    "\n",
    "    \"https://aiatsis.gov.au/explore/articles/indigenous-australian-languages\",\n",
    "    \"https://en.wikipedia.org/wiki/Australian_Aboriginal_languages\",\n",
    "    \"https://www.commonground.org.au/learn/indigenous-languages-avoiding-a-silent-future\",\n",
    "    \"http://theconversation.com/the-state-of-australias-indigenous-languages-and-how-we-can-help-people-speak-them-more-often-109662\",\n",
    "    \"https://www.clc.org.au/articles/info/aboriginal-languages/\",\n",
    "    \"https://www.britannica.com/topic/Australian-Aboriginal-languages\",   \n",
    "\n",
    "    \"https://en.wikipedia.org/wiki/Human_genome\",\n",
    "    \"https://en.wikipedia.org/wiki/Human_Genome_Project\",\n",
    "    \"https://www.nature.com/scitable/topicpage/dna-sequencing-technologies-key-to-the-human-828/\",\n",
    "    \"https://www.britannica.com/event/Human-Genome-Project/Advances-based-on-the-HGP\",\n",
    "    \"https://www.genome.gov/human-genome-project/Completion-FAQ\",\n",
    "    \"https://www.yourgenome.org/stories/how-is-the-completed-human-genome-sequence-being-used\",   \n",
    "\n",
    "    \"https://en.wikipedia.org/wiki/Machine_learning\",\n",
    "    \"https://www.sas.com/en_au/insights/analytics/machine-learning.html\", \n",
    "    \"https://en.wikipedia.org/wiki/London\", \n",
    "    \"https://en.wikipedia.org/wiki/England\",\n",
    "    \"https://en.wikipedia.org/wiki/Titanic_(1997_film)\", \n",
    "    \"https://en.wikipedia.org/wiki/Titanic_(1953_film)\", \n",
    "]\n",
    "nurls = len(urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sc: 0.95 | st: 0.75 | sk: 0.0 | sd: 0.96 | wf: 0.36 | dv: 0.0 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "#-----------------------------------------------------------------------------------------------------------------\n",
    "# Call the functions\n",
    "# Read the HTML contents and lemmatise into two arrays, 't1' and 't2'\n",
    "i = 0\n",
    "j = 2\n",
    "ReadUrls(i, j)\n",
    "# Calculate the cosine similarity between text, titles, key words and descriptions\n",
    "# Some of these will be 0. Not all need to be used.\n",
    "sc = CalculateSimilarity(i,j,nurls)\n",
    "st = SimilarityScore(str(title1),str(title2))\n",
    "sk = SimilarityScore(kw1,kw2)\n",
    "sd = SimilarityScore(desc1,desc2)\n",
    "wf = CalculateWordFrequency()\n",
    "dv = Domain_match(urls[i], urls[j])\n",
    "\n",
    "print('sc:', sc , '|', 'st:', st, '|', 'sk:', sk, '|', 'sd:', sd, '|', 'wf:', wf, '|', 'dv:', dv, '\\n')\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "s = round((sc + wf + dv + st + sk + sd),2)\n",
    "n = 6 # Number of params. Deduct if any is 0\n",
    "sp = round(s/n*100.0,2) # 6 params: body text, word frequency, domain, title, keywords, description\n",
    "print('Score: ', s, 'out of', n, '(', sp , '%)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#\t0\t1\t2\t3\t4\t5\t6\t7\t8\t9\t10\t11\t12\t13\t14\t15\t16\t17\t18\t19\t20\t21\t22\t23\t24\t25\t26\t27\t28\t29\t\n",
      "0\t2.0\t1.64\t1.31\t1.24\t1.41\t1.37\t1.3\t0.91\t0.9\t0.9\t0.89\t1.13\t0.87\t0.83\t0.77\t0.8\t0.77\t0.94\t0.83\t0.94\t1.1\t0.88\t0.89\t0.89\t1.14\t1.13\t0.83\t0.78\t0.77\t0.71\t\n",
      "1\t\t2.0\t1.12\t1.11\t1.38\t1.25\t1.25\t0.88\t0.89\t0.88\t0.84\t0.47\t0.87\t0.82\t0.71\t0.78\t0.7\t1.13\t0.76\t0.93\t0.87\t0.89\t0.86\t0.82\t0.96\t0.47\t0.71\t0.71\t0.72\t0.74\t\n"
     ]
    }
   ],
   "source": [
    "h = \"#\\t\"\n",
    "o = \"\"\n",
    "for i in range (nurls):\n",
    "    h += str(i) + '\\t'\n",
    "#h += '\\n'  \n",
    "print(h)\n",
    "#print('Blah')\n",
    "def AddTabs(i):\n",
    "    global o\n",
    "    for j in range(i):\n",
    "        o += '\\t'\n",
    "for i in range (nurls):\n",
    "    o = str(i)\n",
    "    AddTabs(i+1)\n",
    "    for j in range (i,nurls):\n",
    "        ReadUrls(i, j)\n",
    "        st = SimilarityScore(str(title1),str(title2))\n",
    "        sk = SimilarityScore(kw1,kw2)\n",
    "        sd = SimilarityScore(desc1,desc2)\n",
    "        dv = Domain_match(urls[i], urls[j])\n",
    "        GetDocSegments()\n",
    "        sc = CalculateSimilarity(i,j,nurls)\n",
    "        wf = CalculateWordFrequency()\n",
    "#        s = round((sc + wf + tv + dv),2)\n",
    "#        s = round((sc + wf + dv + st + sk + sd),2)\n",
    "#        s = round(sc,2)\n",
    "        s = round((sc+wf),2)\n",
    "        o += str(s) + '\\t'\n",
    "    print(o)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# __________End of page__________"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
