{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parse a website and say what it is about\n",
    "\n",
    "## INTRODUCTION\n",
    "\n",
    "A web page that is text-centric will give an idea about its content through frequent usage of certain words and phrases. \n",
    "By analying the content of a page for word and phrase frequencies we can deduce the purpose of the page. \n",
    "\n",
    "### Method\n",
    "\n",
    "The web page content is retrieved in a way similar to 'curl' or 'wget'. Only the human readable content is to be analysed. The content is then tokenized and cleaned of common words such as 'a', 'and', 'the', etc. The rest are trimmed to remove endings (e.g. plurals) to form the stems. The frequency of these are calculated and added to an array in descending order. The first 2 to 5 words will usually say what the page is about.\n",
    "\n",
    "By training the dataset using a number of pages of the same theme (e.g. Machine Leraning) it should correctly classify a new page in the same theme.\n",
    "\n",
    "#### Author: Arapaut V. Sivaprasad\n",
    "\n",
    "#### Dates\n",
    "\n",
    "    Created: 26 Oct, 2019.  \n",
    "    Last Modified: 30 Oct, 2019."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import the Python module\n",
    "\n",
    "The NLTK module and the package, 'stopwords', are required in this example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\avs29\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get the web page content\n",
    "\n",
    "By specifying a URL its raw page content is retrieved. Though it returns the html tags like 'title' and 'description', they cannot be trusted fully. The page content must say what the page is about and it can be corroborated with title and description."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "url = \"https://en.wikipedia.org/wiki/Machine_learning\"\n",
    "#url = \"https://en.wikipedia.org/wiki/SpaceX\"\n",
    "#url = \"https://www.webgenie.com\"\n",
    "#response =  urllib.request.urlopen('https://en.wikipedia.org/wiki/SpaceX')\n",
    "response =  urllib.request.urlopen(url)\n",
    "html = response.read()\n",
    "#print(html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get the page title and keep it.\n",
    "\n",
    "Generally the page title will represent the page content, but it cannot be assumed as some may be trying to cheat the search engines. Comparing the title with the page content may give us more confidence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Machine learning - Wikipedia\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "soup = BeautifulSoup(html,'html.parser')\n",
    "text = soup.get_text(strip = True)\n",
    "\n",
    "# Get the document title\n",
    "title = soup.title.string\n",
    "metas = soup.find_all('meta')\n",
    "#print (metas)\n",
    "\n",
    "# To get the 'decription' meta tag. Do not remove.\n",
    "#print ([ meta.attrs['content'] for meta in metas if 'name' in meta.attrs and meta.attrs['name'] == 'description' ])\n",
    "print(title)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split the text into tokens\n",
    "\n",
    "The text is split at white-spaces. This splitting is not always accurate and sometimes can lead to concatenating two words (e.g. 'Insupervised', 'thetraining', 'datafor'). The frequency of this is low and can be ignored. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = [t for t in text.split()]\n",
    "#print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove common words\n",
    "\n",
    "stopwords.words('english') is a file containing a list of words like 'i, me, my, myself, we, ...'. Tokens matching these are removed.\n",
    "\n",
    "'nltk.FreqDist(clean_tokens)' determines the frequency of occurrence of the remaining words. It has been observed that these frequencies are not accurate. However, the disparities are not big to be a concern."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "sr= stopwords.words('english')\n",
    "\n",
    "clean_tokens = tokens[:]\n",
    "for token in tokens:\n",
    "    if token in stopwords.words('english'):\n",
    "        \n",
    "        clean_tokens.remove(token)\n",
    "freq = nltk.FreqDist(clean_tokens)\n",
    "#freq\n",
    "#for key,val in freq.items():\n",
    "#    print(str(key) + ':' + str(val))\n",
    "#freq.plot(20, cumulative=False)\n",
    "#stopwords.words\n",
    "#print (str(freq))\n",
    "#str(freq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make a list of words and their frequencies\n",
    "\n",
    "By appending the frequency number before the word, it will be possible to sort the list numerically. Then, the top x numbers can be chosen for analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['127:learning',\n",
       " '80:machine',\n",
       " '46:data',\n",
       " '40:training',\n",
       " '32:algorithms',\n",
       " '30:Machine',\n",
       " '26:set',\n",
       " '25:used',\n",
       " '22:model',\n",
       " '20:The']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordlist = []\n",
    "for key,val in freq.items():\n",
    "    item = str(val) + ':' + str(key)\n",
    "    wordlist.append(item)\n",
    "wordlist.sort(key=lambda fname: int(fname.split(':')[0]), reverse=True)\n",
    "wordlist[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make anagrams\n",
    "\n",
    "The top 10 are used to make anagrams of the words. The anagram is made from every two consecutive words. This number is arbitrary and we may have to use 3 or more. These are used to find in a hash array the keys that have these words in any order. Another way is to create two hash strings with the words in both orders. The drawback is that there will be one hash for each word, instead of one anagram for 2 words, and that it will not be possible to check one word alone in the hash string. This is important to verify that the anagram detected actually contains both words. There is a possibility that a word may be in an anagram beloging to another phrase, but the chances of both words in the anagram will be lower (though not impossible)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MakeAnagram(w1,w2):\n",
    "# Append the words together and remove duplicate letters\n",
    "    word = ''.join(set(w1+w2)) # Remove duplicates\n",
    "    # Make the anagram and convert all letters to lowercase\n",
    "    anagram = ''.join(sorted(word.lower()))\n",
    "    return anagram\n",
    "#        output = anagram + \" \" + word1[1] + \",\" + word2[1] + \"\\n\"\n",
    "#        f.write(output)\n",
    "#        print(output)\n",
    "#print(anagrams)    "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def MakeAnagram(w1,w2)\n",
    "# Append the words together and remove duplicate letters\n",
    "        word = ''.join(set(word1[1]+word2[1])) # Remove duplicates\n",
    "\n",
    "        # Make the anagram and convert all letters to lowercase\n",
    "        anagram = ''.join(sorted(word.lower()))\n",
    "\n",
    "        # Add to a list, 'anagrams'\n",
    "        anagrams.append(anagram)\n",
    "    \n",
    "        output = anagram + \" \" + word1[1] + \",\" + word2[1] + \"\\n\"\n",
    "        f.write(output)\n",
    "#        print(output)\n",
    "print(anagrams)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['aceghilmnr', 'acdehimnt', 'adginrt', 'aghilmnorst', 'aceghilmmnorst', 'delmosu']\n"
     ]
    }
   ],
   "source": [
    "anagrams = []\n",
    "anagrams_txt = './anagrams.txt'\n",
    "with open(anagrams_txt, 'w+') as f:\n",
    "    for i in range (0,9):\n",
    "        # Take the current and next word from the 'wordlist'\n",
    "        word1 = wordlist[i].split(\":\")\n",
    "        word2 = wordlist[i+1].split(\":\")\n",
    "\n",
    "        n1 = len(word1[1])\n",
    "        if (n1 < 4):\n",
    "            continue\n",
    "\n",
    "        n2 = len(word2[1])\n",
    "        if (n2 < 4):\n",
    "            continue\n",
    "        anagram = MakeAnagram(word1[1], word2[1])\n",
    "\n",
    "        # Add to a list, 'anagrams'\n",
    "        anagrams.append(anagram)\n",
    "        output = anagram + \" \" + word1[1] + \",\" + word2[1] + \"\\n\"\n",
    "        f.write(output)\n",
    "\n",
    "#    f.write(anagrams)\n",
    "print(anagrams)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'aceghilmnr': 'learning,machine',\n",
       " 'acdehimnt': 'machine,data',\n",
       " 'adginrt': 'data,training',\n",
       " 'aghilmnorst': 'training,algorithms',\n",
       " 'aceghilmmnorst': 'algorithms,Machine',\n",
       " 'delmosu': 'used,model'}"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d = {}\n",
    "with open(anagrams_txt) as f:\n",
    "    for line in f:\n",
    "       (key, val) = line.split()\n",
    "       d[key] = val\n",
    "d        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "# See if a word matches with the dict\n",
    "def checkDict(key):\n",
    "#    print (key, \":\", d[key])\n",
    "    return key\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check a word in anagrams \n",
    "\n",
    "To check that a word exists in any anagram in the list. This is not required in the final case. It is required to check that two (or more) words together is in the hash array. For that, the previous cell's calculation of 'anagram' is enough."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell is only to check that a word is in any anagram. Not part of the core program.\n",
    "def checkAnagram(word):\n",
    "    \n",
    "    # Change the search word to lowercase\n",
    "    word = word.lower()\n",
    "    m = len(anagrams)\n",
    "    n = len(word)\n",
    "    if (n < 4):\n",
    "        print(\"Word must be at least 4 chars\")\n",
    "        return\n",
    "    found = 0\n",
    "    for i in range(m):\n",
    "        if (found):\n",
    "            break\n",
    "        l = anagrams[i]\n",
    "        found = 0\n",
    "        \n",
    "        # See that all letters in the word are in the anagram, in any order\n",
    "        for j in range(n):\n",
    "            if(word[j] in l):\n",
    "                found = 1\n",
    "            else:\n",
    "                found = 0\n",
    "                break\n",
    "    if (found == 0):\n",
    "        pass\n",
    "    else:\n",
    "#        print(word,l)\n",
    "        return checkDict(l)\n",
    "#    print(n)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CheckWords(w1, w2):\n",
    "    n1 = len(w1)\n",
    "    n2 = len(w2)\n",
    "    if (n1 < 4 or n2 < 4):\n",
    "        print(\"Word must be at least 4 chars\")\n",
    "        return\n",
    "    anagram = MakeAnagram(w1, w2)\n",
    "#    print(anagram)\n",
    "    try:\n",
    "        if (d[anagram]):\n",
    "            return anagram\n",
    "        else:\n",
    "            return \"Not Found\"\n",
    "    except:\n",
    "            return \"Not Found\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here:  Not Found\n"
     ]
    }
   ],
   "source": [
    "rv = CheckWords('data', 'learning')\n",
    "print(\"Here: \", rv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
