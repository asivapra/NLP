{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parse a website and say what it is about\n",
    "\n",
    "## INTRODUCTION\n",
    "\n",
    "A web page that is text-centric will give an idea about its content through frequent usage of certain words and phrases. \n",
    "By analying the content of a page for word and phrase frequencies we can deduce the purpose of the page. \n",
    "\n",
    "### Method\n",
    "\n",
    "The web page content is retrieved in a way similar to 'curl' or 'wget'. Only the human readable content is to be analysed. The content is then tokenized and cleaned of common words such as 'a', 'and', 'the', etc. The rest are trimmed to remove endings (e.g. plurals) to form the stems. The frequency of these are calculated and added to an array in descending order. The first 2 to 5 words will usually say what the page is about."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import the Python module\n",
    "\n",
    "The NLTK module and the package, 'stopwords', are required in this example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\avs29\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get the web page content\n",
    "\n",
    "By specifying a URL its raw page content is retrieved. Though it returns the html tags like 'title' and 'description', they cannot be trusted fully. The page content must say what the page is about and it can be corroborated with title and description."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "url = \"https://en.wikipedia.org/wiki/Machine_learning\"\n",
    "#url = \"https://en.wikipedia.org/wiki/SpaceX\"\n",
    "#url = \"https://www.webgenie.com\"\n",
    "#response =  urllib.request.urlopen('https://en.wikipedia.org/wiki/SpaceX')\n",
    "response =  urllib.request.urlopen(url)\n",
    "html = response.read()\n",
    "#print(html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get the page title and keep it.\n",
    "\n",
    "Generally the page title will represent the page content, but it cannot be assumed as some may be trying to cheat the search engines. Comparing the title with the page content may give us more confidence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Machine learning - Wikipedia\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "soup = BeautifulSoup(html,'html.parser')\n",
    "text = soup.get_text(strip = True)\n",
    "title = soup.title.string\n",
    "metas = soup.find_all('meta')\n",
    "#print (metas)\n",
    "#print ([ meta.attrs['content'] for meta in metas if 'name' in meta.attrs and meta.attrs['name'] == 'description' ])\n",
    "print(title)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split the text into tokens\n",
    "\n",
    "The text is split at white-spaces. This splitting is not always accurate and sometimes can lead to concatenating two words (e.g. 'Insupervised', 'thetraining', 'datafor'). The frequency of this is low and can be ignored. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = [t for t in text.split()]\n",
    "#print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove common words\n",
    "\n",
    "stopwords.words('english') is a file containing a list of words like 'i, me, my, myself, we, ...'. Tokens matching these are removed.\n",
    "\n",
    "'nltk.FreqDist(clean_tokens)' determines the frequency of occurrence of the remaining words. It has been observed that these frequencies are not accurate. However, the disparities are not big to be a concern."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "sr= stopwords.words('english')\n",
    "\n",
    "clean_tokens = tokens[:]\n",
    "for token in tokens:\n",
    "    if token in stopwords.words('english'):\n",
    "        \n",
    "        clean_tokens.remove(token)\n",
    "freq = nltk.FreqDist(clean_tokens)\n",
    "#freq\n",
    "#for key,val in freq.items():\n",
    "#    print(str(key) + ':' + str(val))\n",
    "#freq.plot(20, cumulative=False)\n",
    "#stopwords.words\n",
    "#print (str(freq))\n",
    "#str(freq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make a list of words and their frequencies\n",
    "\n",
    "By appending the frequency number before the word, it will be possible to sort the list numerically. Then, the top x numbers can be chosen for analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['127:learning',\n",
       " '80:machine',\n",
       " '46:data',\n",
       " '40:training',\n",
       " '32:algorithms',\n",
       " '30:Machine',\n",
       " '26:set',\n",
       " '25:used',\n",
       " '22:model',\n",
       " '20:The']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordlist = []\n",
    "for key,val in freq.items():\n",
    "    item = str(val) + ':' + str(key)\n",
    "    wordlist.append(item)\n",
    "wordlist.sort(key=lambda fname: int(fname.split(':')[0]), reverse=True)\n",
    "wordlist[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make anagrams\n",
    "\n",
    "The top 10 are used to make anagrams of the words. The anagram is made from every two consecutive words. This number is arbitrary and we may have to use 3 or more. The principle is to find in a hash array the keys that have these words in any order."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "anagrams = []\n",
    "for i in range (0,9):\n",
    "    item = wordlist[i]\n",
    "    word = item.split(\":\")\n",
    "    anagram1 = ''.join(set(word[1].lower()))\n",
    "    anagram1 = ''.join(sorted(anagram1))\n",
    "    anagrams.append(anagram)\n",
    "#    print (anagram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['aceghilmnr', 'acdehimnt', 'adginrt', 'aghilmnorst', 'aceghilmmnorst', 'acehimnst', 'destu', 'delmosu', 'dehlmot']\n"
     ]
    }
   ],
   "source": [
    "anagrams = []\n",
    "for i in range (0,9):\n",
    "    # Take the current and next word from the 'wordlist'\n",
    "    word1 = wordlist[i].split(\":\")\n",
    "    word2 = wordlist[i+1].split(\":\")\n",
    "\n",
    "    # Append the words together and remove duplicate letters\n",
    "    word = ''.join(set(word1[1]+word2[1])) # Remove duplicates\n",
    "\n",
    "    # Make the anagram and convert all letters to lowercase\n",
    "    anagram = ''.join(sorted(word.lower()))\n",
    "\n",
    "    # Add to a list, 'anagrams'\n",
    "    anagrams.append(anagram)\n",
    "    \n",
    "print(anagrams)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check a word in anagrams \n",
    "\n",
    "To check that a word exists in any anagram in the list. This is not required in the final case. It is required to check that two (or more) words together is in the hash array. For that, the previous cell's calculation of 'anagram' is enough."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "machine aceghilmnr\n"
     ]
    }
   ],
   "source": [
    "def checkAnagram(word):\n",
    "    m = len(anagrams)\n",
    "    n = len(word)\n",
    "    if (n < 4):\n",
    "        print(\"Word must be at least 4 chars\")\n",
    "        return\n",
    "    found = 0\n",
    "    for i in range(m):\n",
    "        if (found):\n",
    "            break\n",
    "        l = anagrams[i]\n",
    "        found = 0\n",
    "        for j in range(n):\n",
    "            if(word[j] in l):\n",
    "                found = 1\n",
    "            else:\n",
    "                found = 0\n",
    "                break\n",
    "#        print(found)\n",
    "    if (found == 0):\n",
    "        print (\"Not Found:\", word)\n",
    "    else:\n",
    "        print(word,l)\n",
    "#    print(n)\n",
    "ok = checkAnagram(\"machine\") \n",
    "#print (ok)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
