{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parse a website and say what it is about\n",
    "\n",
    "## INTRODUCTION\n",
    "\n",
    "A web page that is text-centric will give an idea about its content through frequent usage of certain words and phrases. \n",
    "By analying the content of a page for word and phrase frequencies we can deduce the purpose of the page. \n",
    "\n",
    "### Method\n",
    "\n",
    "The web page content is retrieved in a way similar to 'curl' or 'wget'. Only the human readable content is to be analysed. The content is then tokenized and cleaned of common words such as 'a', 'and', 'the', etc. The rest are trimmed to remove endings (e.g. plurals) to form the stems. The frequency of these are calculated and added to an array in descending order. The first 2 to 5 words will usually say what the page is about.\n",
    "\n",
    "By training the dataset using a number of pages of the same theme (e.g. Machine Leraning) it should correctly classify a new page in the same theme.\n",
    "\n",
    "### Algorithm\n",
    "\n",
    "- Scan the page content for all words.\n",
    "    - These can include the 'meta-description' but is not included here.\n",
    "- Prepare a word count and calculate the frequency (percentage) of each word among total.\n",
    "    - Sort in descending order\n",
    "- When training, write out the data in a TSV file.\n",
    "    - Only the top 10 words are written.\n",
    "    - This can be editd to add more text.\n",
    "- When testing a new page, do the same to get the word frequencies.\n",
    "- Take the first 10 (arbitrary number) of words.\n",
    "- Read each line in the text file and take the words to compare with the test page words.\n",
    "    - Add up the freqeuncy values in the training data set.\n",
    "    - If the frequency is less than 20 (arbitrary) ignore it.\n",
    "    - Take the line which gives the highest frequency.\n",
    "    - Take its catagory name and report.\n",
    "\n",
    "#### Author: Arapaut V. Sivaprasad\n",
    "\n",
    "#### Dates\n",
    "\n",
    "    Created: 26 Oct, 2019.  \n",
    "    Last Modified: 01 Nov, 2019."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import the Python module\n",
    "\n",
    "The NLTK module and the package, 'stopwords', are required in this example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\avs29\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\avs29\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "#nltk.download()\n",
    "nltk.download('stopwords')\n",
    "nltk.download('words')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get the web page content\n",
    "\n",
    "By specifying a URL its raw page content is retrieved. Though it returns the html tags like 'title' and 'description', they cannot be trusted fully. The page content must say what the page is about and it can be corroborated with title and description."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "url = \"https://en.wikipedia.org/wiki/Machine_learning\"\n",
    "#url = \"https://en.wikipedia.org/wiki/SpaceX\"\n",
    "#url = \"https://www.webgenie.com\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://en.wikipedia.org/wiki/Machine_learning\n",
      "URL already scanned.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "writeout = 1 # If the URL is already proccessed, then dont write it again in catalog.txt\n",
    "with open('catalog.txt', 'r') as f:\n",
    "    content = f.readlines()\n",
    "# you may also want to remove whitespace characters like `\\n` at the end of each line\n",
    "f.close()\n",
    "content = [x.strip() for x in content]\n",
    "lc = len(content)\n",
    "for i in range(lc):\n",
    "    s = content[i].split('\\t')\n",
    "    print (s[0])\n",
    "    if (s[0] == url):\n",
    "        print(\"URL already scanned.\")\n",
    "        writeout = 0\n",
    "        break\n",
    "writeout"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "import re\n",
    "s = \"CamelCase\"\n",
    "s = \"Machine\"\n",
    "p = re.compile('[a-z][A-Z]')\n",
    "\n",
    "try:\n",
    "    res = p.search(s).group(0)\n",
    "except:\n",
    "    pass\n",
    "#res = p.split(s)\n",
    "print(res)\n",
    "n = s.find(res) + 1\n",
    "\n",
    "b = s[:n]\n",
    "e = s[n:]\n",
    "b,e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "response =  urllib.request.urlopen(url)\n",
    "html = response.read()\n",
    "#print(html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get the page title and keep it.\n",
    "\n",
    "Generally the page title will represent the page content, but it cannot be assumed as some may be trying to cheat the search engines. Comparing the title with the page content may give us more confidence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Machine learning - Wikipedia'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from bs4.element import Comment\n",
    "import urllib.request\n",
    "\n",
    "\n",
    "def tag_visible(element):\n",
    "    if element.parent.name in ['style', 'script', 'head', 'title', 'meta', '[document]']:\n",
    "        return False\n",
    "    if isinstance(element, Comment):\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "\n",
    "def text_from_html(body):\n",
    "    soup = BeautifulSoup(body, 'html.parser')\n",
    "    texts = soup.findAll(text=True)\n",
    "    visible_texts = filter(tag_visible, texts)  \n",
    "    return u\" \".join(t.strip() for t in visible_texts)\n",
    "\n",
    "html = urllib.request.urlopen(url).read()\n",
    "text = text_from_html(html)\n",
    "title = soup.title.string\n",
    "title"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "from bs4 import BeautifulSoup\n",
    "soup = BeautifulSoup(html,'html.parser')\n",
    "text = soup.get_text(strip = True)\n",
    "\n",
    "# Get the document title\n",
    "title = soup.title.string\n",
    "metas = soup.find_all('meta')\n",
    "#print (metas)\n",
    "\n",
    "# To get the 'decription' meta tag. Do not remove.\n",
    "#print ([ meta.attrs['content'] for meta in metas if 'name' in meta.attrs and meta.attrs['name'] == 'description' ])\n",
    "print(title)\n",
    "#text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split the text into tokens\n",
    "\n",
    "The text is split at white-spaces. This splitting is not always accurate and sometimes can lead to concatenating two words (e.g. 'Insupervised', 'thetraining', 'datafor'). The frequency of this is low and can be ignored. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "tokens = []\n",
    "t1 = []\n",
    "t2 = []\n",
    "p = re.compile('[-_\\\", {};:=?\\[\\]\\(\\)\\'.]')\n",
    "t = p.split(text)\n",
    "#print(t)\n",
    "lt = len(t)\n",
    "for i in range(lt):\n",
    "    w = t[i]\n",
    "    lw = len(w)\n",
    "    if(lw > 3):\n",
    "        t1.append(w)\n",
    "lt = len(t1)\n",
    "q = re.compile('[a-z][A-Z]')\n",
    "for i in range (lt):\n",
    "    s = t1[i]\n",
    "#    print(s)\n",
    "    p = re.compile('[a-z][A-Z]')\n",
    "    try:\n",
    "        res = p.search(s).group(0)\n",
    "    except:\n",
    "        continue\n",
    "#    print(res)\n",
    "    n = s.find(res) + 1\n",
    "    b = s[:n]\n",
    "    e = s[n:]\n",
    "    t1[i] = ''\n",
    "    t2.append(b)\n",
    "    t2.append(e)\n",
    "for i in range(lt):\n",
    "    w = t[i]\n",
    "    lw = len(w)\n",
    "    if(lw > 3):\n",
    "        tokens.append(w)\n",
    "tokens += t2\n",
    "#tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokens = [t for t in text.split()]\n",
    "lt = len(tokens)\n",
    "for i in range(lt):\n",
    "    res = p.split(tokens[i])\n",
    "#    tokens[i] = tokens[i].replace('[\\\"]', ' ')\n",
    "#print(tokens[i])\n",
    "\n",
    "#print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove common words\n",
    "\n",
    "stopwords.words('english') is a file containing a list of words like 'i, me, my, myself, we, ...'. Tokens matching these are removed.\n",
    "\n",
    "'nltk.FreqDist(clean_tokens)' determines the frequency of occurrence of the remaining words. It has been observed that these frequencies are not accurate. However, the disparities are not big to be a concern."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "sr= stopwords.words('english')\n",
    "\n",
    "clean_tokens = tokens[:]\n",
    "for token in tokens:\n",
    "    if token in stopwords.words('english'):\n",
    "        token = token.lower()\n",
    "        clean_tokens.remove(token)\n",
    "freq = nltk.FreqDist(clean_tokens)\n",
    "#freq.items()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make a list of words and their frequencies\n",
    "\n",
    "By appending the frequency number before the word, it will be possible to sort the list numerically. Then, the top x numbers can be chosen for analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a list of words of 4+ chars to compare with tokens from the web page\n",
    "from nltk.corpus import words\n",
    "english_words = words.words()\n",
    "english_dict = {}\n",
    "for word in (english_words):\n",
    "    n = len(word)\n",
    "    if (n > 3):\n",
    "        english_dict[word] = 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['178:learning',\n",
       " '65:data',\n",
       " '52:machine',\n",
       " '32:training',\n",
       " '26:model',\n",
       " '22:edit',\n",
       " '20:artificial',\n",
       " '19:used',\n",
       " '19:also',\n",
       " '16:classification']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Count all words on the page and sort. Only the words in english_dict are taken.\n",
    "wordlist = []\n",
    "for key,val in freq.items():\n",
    "    item = str(val) + ':' + str(key.lower())\n",
    "    try:\n",
    "#        print(str(key))\n",
    "        if(english_dict[str(key)]):\n",
    "            wordlist.append(item)\n",
    "    except:\n",
    "        pass\n",
    "wordlist.sort(key=lambda fname: int(fname.split(':')[0]), reverse=True)\n",
    "wordlist[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine the duplicates into a single word and add their counts together\n",
    "combined_words = []\n",
    "lw = len(wordlist)\n",
    "for i in range(lw):\n",
    "    if(wordlist[i] == \"\"):\n",
    "        continue\n",
    "    fields = wordlist[i].split(':')\n",
    "    c1 = int(fields[0])\n",
    "    w1  = fields[1]\n",
    "    item0 = str(c1) + \":\" + w1\n",
    "    for j in range(i+1, lw):\n",
    "        if(wordlist[j] == \"\"):\n",
    "            continue\n",
    "        fields1 = wordlist[j].split(':')\n",
    "        c2 = int(fields1[0])\n",
    "        w2  = fields1[1]\n",
    "        if (w1 == w2):\n",
    "            wordlist[j] = \"\"\n",
    "            w1 = w2\n",
    "            c1 = int(c1)+int(c2)\n",
    "            item = str(c1) + \":\" + w1\n",
    "            item0 = item\n",
    "    combined_words.append(item0)\n",
    "#    wordlist = combined_words   \n",
    "#print(combined_words)\n",
    "wordlist = combined_words   \n",
    "#print(wordlist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OK to write\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2389"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if(writeout):\n",
    "    print(\"OK to write\")\n",
    "    # Write out this web page's data into a TSV file \n",
    "    wordfreq = \"\"\n",
    "    lw = len(wordlist) - 1\n",
    "    tot = 0\n",
    "    with open('catalog.txt', 'a') as file:\n",
    "\n",
    "        # Count the total number of words\n",
    "        for i in range (0,lw):\n",
    "            fields = wordlist[i].split(\":\")\n",
    "            tot += int(fields[0])\n",
    "        for i in range (0,10):\n",
    "            fields = wordlist[i].split(\":\")\n",
    "            c = int(fields[0])\n",
    "            w = fields[1]\n",
    "    #        f = str(int(c/tot*100+0.5))\n",
    "            f = str(round(float(c/tot*100),2))\n",
    "    #        print(c,f)\n",
    "            fw = f + \"-\" + w\n",
    "            wordfreq += fw + \" \"\n",
    "            category = \"Space Exploration\"\n",
    "            line = url + \"\\t\" + wordfreq + \"\\t\" + title + \"\\t\" + category + \"\\n\"\n",
    "        file.write(line)\n",
    "    file.close()\n",
    "else:\n",
    "    print(\"Already written. Not writing out again\")\n",
    "\n",
    "tot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
